#!/usr/bin/env python3
"""
üîÆ Fetch Trending Mystery Topics for Mythica Report
v6.0.1: Fixed entity-based duplicate detection (removed false positives)

CHANGES IN v6.0.1:
- Tightened entity extraction to only proper nouns and specific identifiers
- Added verb phrase blacklist (prevents "its creators vanished" from being treated as entity)
- Removed fallback generic word joining (prevented fake entities)
- Whitelist-only approach for known cases/places
"""

import json
import time
import random
import os
import re
from typing import List, Dict, Any
from datetime import datetime, timedelta
import google.generativeai as genai
import requests
from bs4 import BeautifulSoup
import argparse

# ========================================================================
# CATEGORY-AWARE FETCHING (v6.0.9)
# ========================================================================
parser = argparse.ArgumentParser(description='Fetch trending mystery topics for Mythica Report')
parser.add_argument(
    '--category', 
    type=str, 
    default='general',
    choices=['general', 'disturbing_medical', 'dark_experiments', 'dark_history', 
             'disappearance', 'phenomena', 'crime', 'conspiracy'],
    help='Mystery category to fetch trending topics for'
)
args = parser.parse_args()

MYSTERY_CATEGORY = args.category

print(f"üéØ CATEGORY-AWARE FETCH: {MYSTERY_CATEGORY}")

genai.configure(api_key=os.getenv("GEMINI_API_KEY"))

try:
    models = genai.list_models()
    model_name = None
    for m in models:
        if 'generateContent' in m.supported_generation_methods:
            if '2.5-flash' in m.name or '2.0-flash' in m.name:
                model_name = m.name
                break
            elif '1.5-flash' in m.name and not model_name:
                model_name = m.name
    
    if not model_name:
        model_name = "models/gemini-1.5-flash"
    
    print(f"‚úÖ Using model: {model_name}")
    model = genai.GenerativeModel(model_name)
except Exception as e:
    print(f"‚ö†Ô∏è Error listing models: {e}")
    model = genai.GenerativeModel("models/gemini-1.5-flash")

TMP = os.getenv("GITHUB_WORKSPACE", ".") + "/tmp"
os.makedirs(TMP, exist_ok=True)

def load_history():
    """Load content history from previous runs"""
    history_file = os.path.join(TMP, "content_history.json")
    if os.path.exists(history_file):
        try:
            with open(history_file, 'r', encoding='utf-8') as f:
                history = json.load(f)
                print(f"üìÇ Loaded {len(history.get('topics', []))} topics from history")
                return history
        except Exception as e:
            print(f"‚ö†Ô∏è Could not load history: {e}")
            return {'topics': [], 'version': '6.0.1_entity_fix'}
    
    print("üìÇ No previous history found, starting fresh")
    return {'topics': [], 'version': '6.0.1_entity_fix'}

def get_category_search_terms(category: str) -> List[str]:
    """
    üÜï v6.0.9: Get category-specific search terms for trending APIs
    
    Returns list of search queries optimized for each mystery category
    """
    
    category_terms = {
        'disturbing_medical': [
            # Medical conditions
            'rare disease mystery',
            'fatal familial insomnia',
            'mysterious illness',
            'kuru disease',
            'dancing plague',
            'mass hysteria medical',
            'unexplained syndrome',
            'medical anomaly',
            
            # Historical medical
            'radium girls',
            'minamata disease',
            'encephalitis lethargica',
            'tanganyika laughing epidemic',
            'ergotism',
            
            # Body horror
            'fibrodysplasia ossificans progressiva',
            'turning to stone disease',
            'mystery paralysis',
            'sleeping sickness',
            'radiation poisoning victims'
        ],
        
        'dark_experiments': [
            # Government experiments
            'mk ultra',
            'mkultra declassified',
            'stanford prison experiment',
            'milgram experiment',
            'tuskegee experiment',
            'unit 731',
            
            # Secret research
            'secret government experiments',
            'cia mind control',
            'project artichoke',
            'project bluebird',
            'midnight climax',
            'edgewood arsenal',
            
            # Psychological
            'unethical human experiments',
            'sleep deprivation experiment',
            'russian sleep experiment',
            'psychological torture experiments',
            'behavior modification research'
        ],
        
        'dark_history': [
            # Historical mysteries
            'dark historical events',
            'mysterious 1800s incident',
            'dark day 1780',
            'mysterious historical phenomenon',
            
            # Specific events
            'radium girls story',
            'dancing plague 1518',
            'strasbourg dancing mania',
            'kentucky meat shower',
            'roanoke colony mystery',
            
            # Time periods
            'victorian mysteries',
            'medieval unexplained events',
            'colonial era mysteries',
            'industrial revolution disasters',
            '19th century mysteries',
            'historical mass poisoning'
        ],
        
        'disappearance': [
            # Classic disappearances
            'missing person mystery',
            'unsolved disappearance',
            'vanished without trace',
            
            # Famous cases
            'dyatlov pass incident',
            'db cooper mystery',
            'amelia earhart disappearance',
            'flight 19 bermuda triangle',
            'mh370 mystery',
            'maura murray case',
            
            # Categories
            'national park disappearances',
            'missing 411',
            'vanished hikers',
            'missing climbers',
            'lost at sea mysteries',
            'ghost ship mary celeste'
        ],
        
        'phenomena': [
            # Space/cosmic
            'wow signal',
            'mysterious space signal',
            'fast radio burst',
            'unexplained radio transmission',
            
            # Lights
            'hessdalen lights',
            'marfa lights',
            'brown mountain lights',
            'ufo sighting',
            'uap phenomenon',
            
            # Sounds
            'the hum phenomenon',
            'taos hum',
            'skyquake mystery',
            'bloop sound',
            'underwater sounds',
            
            # Paranormal
            'numbers stations',
            'mysterious broadcasts',
            'unexplained phenomenon',
            'paranormal activity',
            'glitch in the matrix stories'
        ],
        
        'crime': [
            # Serial killers
            'zodiac killer',
            'zodiac cipher',
            'jack the ripper',
            'golden state killer',
            
            # Unsolved murders
            'jonbenet ramsey case',
            'black dahlia murder',
            'somerton man mystery',
            'boy in the box case',
            
            # Cold cases
            'unsolved murder mystery',
            'cold case files',
            'true crime unsolved',
            'mysterious death case',
            'murder mystery unsolved'
        ],
        
        'conspiracy': [
            # Classic conspiracies
            'area 51 secrets',
            'philadelphia experiment',
            'montauk project',
            
            # Cover-ups
            'government coverup',
            'classified documents leaked',
            'conspiracy theory',
            'unexplained government activity'
        ],
        
        'general': [
            # Broad mystery terms
            'unsolved mysteries',
            'strange disappearances',
            'unexplained phenomena',
            'creepy stories',
            'historical mysteries',
            'internet mysteries',
            'true crime mysteries'
        ]
    }
    
    selected_terms = category_terms.get(category, category_terms['general'])
    
    print(f"   üîç Using {len(selected_terms)} category-specific search terms for '{category}'")
    
    return selected_terms

def get_category_subreddits(category: str) -> List[str]:
    """
    üÜï v6.0.9: Get category-specific subreddits
    
    Returns list of subreddits optimized for each mystery category
    """
    
    category_subreddits = {
        'disturbing_medical': [
            'UnresolvedMysteries',
            'creepy',
            'medizzy',
            'morbidquestions',
            'TrueCreepy',
            'nosleep',  # fictional but sometimes based on real conditions
            'medical'
        ],
        
        'dark_experiments': [
            'UnresolvedMysteries',
            'conspiracy',
            'TrueCrime',
            'history',
            'todayilearned',
            'psychology'
        ],
        
        'dark_history': [
            'history',
            'HistoryMemes',
            'UnresolvedMysteries',
            'creepy',
            'TrueHistoricalMystery',
            'historicalrage'
        ],
        
        'disappearance': [
            'UnresolvedMysteries',
            'Missing411',
            'TrueCrime',
            'RBI',
            'mystery',
            'UnsolvedMurders'
        ],
        
        'phenomena': [
            'HighStrangeness',
            'Glitch_in_the_Matrix',
            'UnexplainedPhotos',
            'Paranormal',
            'UFOs',
            'astronomy',
            'space'
        ],
        
        'crime': [
            'TrueCrime',
            'UnresolvedMysteries',
            'UnsolvedMurders',
            'serialkillers',
            'ColdCases',
            'mystery'
        ],
        
        'conspiracy': [
            'conspiracy',
            'UnresolvedMysteries',
            'HighStrangeness',
            'AlternativeHistory'
        ],
        
        'general': [
            'UnsolvedMysteries',
            'HighStrangeness',
            'Glitch_in_the_Matrix',
            'creepy',
            'RBI',
            'TrueCrime'
        ]
    }
    
    selected_subs = category_subreddits.get(category, category_subreddits['general'])
    
    print(f"   üì± Using {len(selected_subs)} category-specific subreddits for '{category}'")
    
    return selected_subs

def get_google_trends_mystery() -> List[str]:
    """Get real trending mystery-related searches from Google Trends"""
    try:
        from pytrends.request import TrendReq
        
        print(f"üîÆ Fetching Google Trends (Unsolved Mysteries & Strange Phenomena)...")
        
        try:
            pytrends = TrendReq(hl='en-US', tz=360, timeout=(10, 25))
        except Exception as init_error:
            print(f"   ‚ö†Ô∏è PyTrends initialization failed: {init_error}")
            return []
        
        relevant_trends = []
        
        mystery_topics = get_category_search_terms(MYSTERY_CATEGORY)
        print(f"   üéØ Searching Google Trends for '{MYSTERY_CATEGORY}' topics...")
        
        for topic in mystery_topics:
            try:
                print(f"   üîç Searching trends for: {topic}")
                pytrends.build_payload([topic], timeframe='now 7-d', geo='US')
                
                related = pytrends.related_queries()
                
                if topic in related and 'top' in related[topic]:
                    top_queries = related[topic]['top']
                    if top_queries is not None and not top_queries.empty:
                        for query in top_queries['query'].head(5):
                            if is_mystery_query(query):
                                relevant_trends.append(query)
                                print(f"      ‚úì {query}")
                
                if topic in related and 'rising' in related[topic]:
                    rising_queries = related[topic]['rising']
                    if rising_queries is not None and not rising_queries.empty:
                        for query in rising_queries['query'].head(3):
                            if is_mystery_query(query):
                                relevant_trends.append(f"{query} (üî• RISING)")
                                print(f"      üî• {query} (RISING)")
                
                time.sleep(random.uniform(1.5, 3.0))
                
            except Exception as e:
                print(f"   ‚ö†Ô∏è Failed for '{topic}': {str(e)[:50]}...")
                continue
        
        print(f"‚úÖ Found {len(relevant_trends)} mystery trends from Google")
        return relevant_trends[:20]
        
    except ImportError:
        print("‚ö†Ô∏è pytrends not installed - skipping Google Trends")
        return []
    except Exception as e:
        print(f"‚ö†Ô∏è Google Trends failed: {e}")
        return []


def is_mystery_query(query: str) -> bool:
    """Filter for mystery/unsolved story relevance"""
    query_lower = query.lower()
    
    good_keywords = [
        'mystery', 'unsolved', 'disappearance', 'creepy', 'unexplained', 'strange',
        'case', 'theory', 'phenomenon', 'haunting', 'secret', 'code', 'lost',
        'ghost', 'alien', 'ufo', 'glitch', 'sighting'
    ]
    bad_keywords = [
        'movie', 'trailer', 'explained', 'ending', 'game', 'review',
        'full episode', 'cast', 'soundtrack', 'price', 'buy', 'shop', 'sale',
        'motivation', 'workout'
    ]
    
    has_good = any(kw in query_lower for kw in good_keywords)
    has_bad = any(kw in query_lower for kw in bad_keywords)
    
    return has_good and not has_bad and len(query) > 10


def get_reddit_mystery_trends() -> List[str]:
    """Get trending posts from mystery subreddits (optimized to avoid zero results)"""
    try:
        print("üîÆ Fetching Reddit mystery trends...")

        # Expanded subreddit list
        subreddits = get_category_subreddits(MYSTERY_CATEGORY)
        
        print(f"   üéØ Searching Reddit for '{MYSTERY_CATEGORY}' topics...")

        trends = []

        # Randomize 4 subreddits per run
        sample_size = min(4, len(subreddits))
        for subreddit in random.sample(subreddits, sample_size):
            urls = [
                f'https://www.reddit.com/r/{subreddit}/hot.json?limit=25',
                f'https://www.reddit.com/r/{subreddit}/new.json?limit=25',
                f'https://www.reddit.com/r/{subreddit}/rising.json?limit=25'
            ]

            posts_found = 0

            for url in urls:
                for attempt in range(3):
                    try:
                        headers = {'User-Agent': 'Mozilla/5.0'}
                        response = requests.get(url, headers=headers, timeout=10)
                        if response.status_code != 200:
                            raise ValueError(f"Status code {response.status_code}")
                        data = response.json()
                        break
                    except Exception as e:
                        print(f"   ‚ö†Ô∏è Attempt {attempt + 1} failed for {url}: {e}")
                        time.sleep(2 ** attempt)
                else:
                    continue  # skip to next URL if all attempts fail

                for post in data.get('data', {}).get('children', [])[:20]:
                    post_data = post.get('data', {})
                    title = post_data.get('title', '')
                    upvotes = post_data.get('ups', 0)

                    # Loosen good phrases to capture more trends
                    good_phrases = [
                        'what happened to', 'the strange case of', 'the mystery of',
                        'unsolved disappearance', 'the only clue', 'chilling story',
                        'a strange detail about', 'nobody can explain', 'what was the',
                        'disappearance', 'vanished', 'mystery'
                    ]
                    bad_phrases = [
                        'my theory', 'what do you think', 'discussion', 'help me find',
                        'rant', 'meta', 'ama', 'unpopular opinion'
                    ]

                    title_lower = title.lower()
                    has_good = any(phrase in title_lower for phrase in good_phrases)
                    has_bad = any(phrase in title_lower for phrase in bad_phrases)
                    is_viral = upvotes >= 100  # lower threshold

                    if (has_good or is_viral) and not has_bad:
                        clean_title = clean_reddit_title(title)
                        if clean_title and len(clean_title) >= 15:
                            trends.append(clean_title)
                            posts_found += 1
                            print(f"      ‚úì ({upvotes} ‚Üë) {clean_title[:70]}")

                time.sleep(random.uniform(1.5, 3.0))

            print(f"   Found {posts_found} mystery story leads in r/{subreddit}")

        print(f"‚úÖ Found {len(trends)} total trends from Reddit")
        return trends[:20]

    except Exception as e:
        print(f"‚ö†Ô∏è Reddit scraping failed: {e}")
        return []


def clean_reddit_title(title: str) -> str:
    """Clean Reddit post titles"""
    title = re.sub(r'\[.*?\]', '', title)
    title = re.sub(r'!!!+', '!', title)
    title = re.sub(r'\?\?+', '?', title)
    title = re.sub(r'[^\w\s\-.,!?\'"():;]', '', title)
    return title.strip()


def get_youtube_mystery_trends() -> List[str]:
    """Scrape trending mystery videos from YouTube"""
    try:
        print("üîÆ Fetching YouTube trending mystery videos...")
        
        # üÜï v6.0.9: Use category-specific search queries
        category_terms = get_category_search_terms(MYSTERY_CATEGORY)
        
        # Select top 5 most specific terms for YouTube
        search_queries = category_terms[:5]
        
        print(f"   üéØ Searching YouTube for '{MYSTERY_CATEGORY}' topics...")
        
        trends = []
        
        for query in search_queries:
            try:
                search_url = f"https://www.youtube.com/results?search_query={query.replace(' ', '+')}&sp=CAMSAhAB"
                headers = {'User-Agent': 'Mozilla/5.0'}
                
                print(f"   üé• Searching: {query}")
                response = requests.get(search_url, headers=headers, timeout=10)
                
                if response.status_code == 200:
                    title_pattern = r'"title":{"runs":\[{"text":"([^"]+)"}\]'
                    matches = re.findall(title_pattern, response.text)
                    
                    found_count = 0
                    for title in matches[:10]:
                        if is_mystery_title(title):
                            trends.append(title)
                            found_count += 1
                            print(f"      ‚úì {title[:70]}")
                    print(f"      Found {found_count} videos")
                
                time.sleep(random.uniform(2.0, 3.0))
                
            except Exception as e:
                print(f"   ‚ö†Ô∏è Failed for '{query}': {e}")
                continue
        
        print(f"‚úÖ Found {len(trends)} trends from YouTube")
        return trends[:15]
        
    except Exception as e:
        print(f"‚ö†Ô∏è YouTube scraping failed: {e}")
        return []


def is_mystery_title(title: str) -> bool:
    """Check if YouTube title is mystery content"""
    title_lower = title.lower()
    good_keywords = [
        'mystery', 'unsolved', 'creepy', 'strange', 'disappearance',
        'terrifying', 'unexplained', 'case of', 'chilling', 'haunting'
    ]
    bad_keywords = [
        'react', 'reaction', 'review', 'analysis', 'breakdown',
        'interview', 'compilation', 'playlist', 'top 10'
    ]
    
    has_good = any(kw in title_lower for kw in good_keywords)
    has_bad = any(kw in title_lower for kw in bad_keywords)
    
    return has_good and not has_bad and len(title) > 20


def get_evergreen_mystery_themes() -> List[str]:
    """Classic mystery topics"""
    evergreen = [
        "The Vanishing of the Mary Celeste Crew",
        "The Uncrackable Code of the Zodiac Killer",
        "Who Was D.B. Cooper? The Skyjacker Who Disappeared",
        "The Lost Colony of Roanoke: A 400-Year-Old Mystery",
        "The Chilling Case of the Dyatlov Pass Incident",
        "The Wow! Signal: A Message From Deep Space?",
        "The Mystery of the Bermuda Triangle's Vanishing Ships",
        "The Tunguska Event: The Day a Forest Was Flattened",
        "What is The Hum? The Unexplained Sound Heard Worldwide",
        "The Quest to Solve Cicada 3301: The Internet's Hardest Puzzle",
        "The Eerie Last Online Posts of People Who Vanished",
        "Lake City Quiet Pills: The Cryptic Reddit Mystery",
        "Numbers Stations: Ghostly Radio Broadcasts",
        "Lost Treasures That Are Still Waiting To Be Found",
        "The World's Most Mysterious Books That No One Can Read"
    ]
    
    print(f"‚úÖ Loaded {len(evergreen)} evergreen mystery themes")
    return evergreen


def get_real_mystery_trends() -> List[str]:
    """Combine multiple sources for trending topics"""
    
    print("\n" + "="*70)
    print("üîÆ FETCHING REAL-TIME MYSTERY TRENDS FOR MYTHICA REPORT")
    print("="*70)
    
    all_trends = []
    source_counts = {}
    
    try:
        google_trends = get_google_trends_mystery()
        all_trends.extend(google_trends)
        source_counts['Google Trends'] = len(google_trends)
    except Exception as e:
        print(f"‚ö†Ô∏è Google Trends error: {e}")

    try:
        reddit_trends = get_reddit_mystery_trends()
        all_trends.extend(reddit_trends)
        source_counts['Reddit'] = len(reddit_trends)
    except Exception as e:
        print(f"‚ö†Ô∏è Reddit error: {e}")

    try:
        youtube_trends = get_youtube_mystery_trends()
        all_trends.extend(youtube_trends)
        source_counts['YouTube'] = len(youtube_trends)
    except Exception as e:
        print(f"‚ö†Ô∏è YouTube error: {e}")
    
    evergreen = get_evergreen_mystery_themes()
    all_trends.extend(evergreen)
    source_counts['Evergreen'] = len(evergreen)
    
    seen = set()
    unique_trends = []
    for trend in all_trends:
        trend_clean = trend.lower().strip()
        is_duplicate = False
        for seen_trend in seen:
            if similar_strings(trend_clean, seen_trend) > 0.8:
                is_duplicate = True
                break
        
        if not is_duplicate and len(trend) > 15:
            seen.add(trend_clean)
            unique_trends.append(trend)
    
    print(f"\nüìä TREND SOURCES SUMMARY:")
    for source, count in source_counts.items():
        print(f"   ‚Ä¢ {source}: {count} topics")
    print(f"\n   TOTAL UNIQUE: {len(unique_trends)} mystery trends")
    
    return unique_trends[:30]


def similar_strings(s1: str, s2: str) -> float:
    """Calculate similarity between strings"""
    words1 = set(re.findall(r'\w+', s1.lower()))
    words2 = set(re.findall(r'\w+', s2.lower()))
    if not words1 or not words2: return 0.0
    intersection = len(words1 & words2)
    union = len(words1 | words2)
    return intersection / union if union > 0 else 0.0


def extract_entities_from_title(title: str) -> set:
    """
    Extract key entities (names, events, codes, numbers) from video titles
    v6.0.3: Fixed 3-word phrase extraction (blacklisted all generic patterns)
    
    STRICT RULES:
    - Only proper nouns BEFORE colon (person/place names)
    - Only specific identifiers (flight numbers, codes)
    - Only known whitelisted entities (case-insensitive)
    - NO verb phrases, NO generic descriptors, NO post-colon content
    
    Returns normalized entity set for duplicate detection.
    """
    import re
    
    entities = set()
    
    # ========================================================================
    # STEP 1: Split title at colon (only extract from BEFORE colon)
    # ========================================================================
    title_before_colon = title.split(':')[0].strip()
    clean_title = title.lower()
    
    # ========================================================================
    # STEP 2: Known entity whitelist (case-insensitive search)
    # ========================================================================
    known_entities = {
        # ‚úÖ PEOPLE
        'asha degree': 'ashadegree',
        'natasha ryan': 'natasharyan',
        'elisa lam': 'elisalam',
        'maura murray': 'mauramurray',
        'jonbenet ramsey': 'jonbenetramsey',
        'jonbenet': 'jonbenetramsey',
        'jimmy hoffa': 'jimmyhoffa',
        'amelia earhart': 'ameliaearhart',
        'db cooper': 'dbcooper',
        'd.b. cooper': 'dbcooper',
        'leah roberts': 'leahroberts',
        'brandon swanson': 'brandonswanson',
        'brian shaffer': 'brianshaffer',
        'brandon lawson': 'brandonlawson',
        'rey rivera': 'reyrivera',
        
        # ‚úÖ EVENTS/CASES
        'flight 19': 'flight19',
        'mh370': 'mh370',
        'mh 370': 'mh370',
        'zodiac': 'zodiac',
        'zodiac killer': 'zodiac',
        'dyatlov pass': 'dyatlovpass',
        'tunguska': 'tunguska',
        'tunguska event': 'tunguska',
        'wow signal': 'wowsignal',
        'cicada 3301': 'cicada3301',
        'lake city quiet pills': 'lakecityquietpills',
        
        # ‚úÖ PLACES
        'roanoke': 'roanoke',
        'roanoke colony': 'roanoke',
        'bermuda triangle': 'bermudatriangle',
        'skinwalker ranch': 'skinwalkerranch',
        'mary celeste': 'maryceleste',
        'flannan isles': 'flannanisles',
        'alcatraz': 'alcatraz',
        'area 51': 'area51',
        
        # ‚úÖ EXPERIMENTS
        'mk ultra': 'mkultra',
        'mkultra': 'mkultra',
        'stanford prison': 'stanfordprison',
        'tuskegee': 'tuskegee',
        'unit 731': 'unit731'
    }
    
    for known_name, normalized in known_entities.items():
        if known_name in clean_title:
            entities.add(normalized)
    
    # ========================================================================
    # STEP 3: Extract proper names ONLY from before colon
    # ========================================================================
    # Pattern matches 2-3 word capitalized phrases
    name_pattern = r'\b([A-Z][a-z]+(?:\s+[A-Z][a-z]+){1,2})\b'
    
    # ‚ùå COMPREHENSIVE BLACKLIST: All generic 2-3 word phrases
    blacklist = {
        # 2-word generic descriptors
        'the girl', 'the child', 'the teenager', 'the woman', 'the man',
        'the boy', 'the student', 'the worker', 'the driver', 'the patient',
        'the hiker', 'the tourist', 'the hijacker', 'the scientist',
        'the union boss', 'the family', 'the crew', 'the colony',
        'the posts', 'the riches', 'the signal', 'the experiment',
        'the project', 'the case', 'the treasure', 'the code',
        'the mystery', 'the files', 'the broadcasts', 'the space',
        'the ghost', 'the ship', 'the plane', 'the flight',
        
        # 3-word generic phrases (THE PROBLEM!)
        'the teenager who', 'the girl who', 'the woman who', 'the man who',
        'the mystery of', 'the case of', 'the story of', 'the tale of',
        'the posts that', 'the riches that', 'the signal that', 'the colony that',
        'the experiment that', 'the space signal', 'the ghost ship',
        'the lost colony', 'the vanishing of', 'the disappearance of',
        'vanished with their', 'vanished from history', 'turned people to',
        'the question of', 'the secret of', 'the truth about',
        
        # Verb phrases
        'who vanished', 'that vanished', 'which vanished', 'who disappeared',
        'turned people', 'came from', 'went to', 'found in',
        'seen in', 'heard in', 'discovered in'
    }
    
    for match in re.finditer(name_pattern, title_before_colon):
        entity = match.group(1).lower()
        
        if entity not in blacklist:
            # Normalize: "Sarah Johnson" ‚Üí "sarahjohnson"
            normalized_entity = entity.replace(' ', '')
            
            # Avoid duplicating whitelist entries
            # Check if this entity is already covered by whitelist
            already_whitelisted = False
            for known_key in known_entities.keys():
                if known_key.replace(' ', '') == normalized_entity:
                    already_whitelisted = True
                    break
            
            if not already_whitelisted:
                entities.add(normalized_entity)
    
    # ========================================================================
    # STEP 4: Flight/case numbers (anywhere in title)
    # ========================================================================
    flight_pattern = r'\b(flight\s*\d+|mh\s*\d+|ua\s*\d+|pan\s*am\s*\d+)\b'
    for match in re.finditer(flight_pattern, clean_title):
        entity = match.group(1).replace(' ', '')
        entities.add(entity)
    
    # ========================================================================
    # STEP 5: Codes and signals (anywhere in title)
    # ========================================================================
    code_pattern = r'\b(cicada\s*\d+|wow\s+signal|lake\s+city\s+quiet\s+pills)\b'
    for match in re.finditer(code_pattern, clean_title):
        entity = match.group(1).replace(' ', '')
        entities.add(entity)
    
    return entities

def load_entity_history(history: dict, days: int = 90) -> set:
    """
    Load all entities from recent history
    
    Args:
        history: Content history dict from load_history()
        days: How far back to check (default 90 days)
    
    Returns:
        Set of normalized entity strings
    """
    from datetime import datetime, timedelta
    
    entity_set = set()
    cutoff_date = datetime.now() - timedelta(days=days)
    
    for topic_entry in history.get('topics', []):
        # Check if topic is recent enough
        topic_date_str = topic_entry.get('date')
        if topic_date_str:
            try:
                topic_date = datetime.fromisoformat(topic_date_str)
                if topic_date < cutoff_date:
                    continue  # Skip old topics
            except:
                pass  # If date parsing fails, include it to be safe
        
        # Extract entities from historical title
        title = topic_entry.get('title', '')
        if title:
            entities = extract_entities_from_title(title)
            entity_set.update(entities)
    
    print(f"üìÇ Loaded {len(entity_set)} unique entities from last {days} days")
    if entity_set:
        # Show sample entities for debugging
        sample = sorted(list(entity_set))[:10]
        print(f"   Sample entities: {sample}")
    
    return entity_set


def has_duplicate_entity(new_title: str, entity_history: set) -> tuple:
    """
    Check if new topic contains entities that already exist in history
    
    Args:
        new_title: Title to check
        entity_history: Set of historical entities
    
    Returns:
        (is_duplicate: bool, matching_entities: list)
    """
    new_entities = extract_entities_from_title(new_title)
    
    if not new_entities:
        # If no entities extracted, allow it (topic is generic/new angle)
        return False, []
    
    # Check each new entity against history
    matches = []
    for entity in new_entities:
        if entity in entity_history:
            matches.append(entity)
    
    # Only block if there's at least one entity match
    if matches:
        return True, matches
    
    return False, []


def filter_and_rank_mystery_trends(trends: List[str], history: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    üö® ENHANCED v6.0.1: Fixed entity-based duplicate detection
    - Prevents posting same person/event/case multiple times
    - Checks last 90 days of history
    - Blocks at topic selection stage (before script generation)
    - v6.0.1: Removed false positives from verb phrases
    """
    
    if not trends:
        print("‚ö†Ô∏è No trends to filter, using fallback...")
        return get_fallback_mystery_ideas()
    
    print(f"\nü§ñ Using Gemini to rank {len(trends)} mystery topics (ENTITY-AWARE v6.0.1)...")

    # Load entity history (last 90 days)
    entity_history = load_entity_history(history, days=90)
    
    previous_titles = [item.get('title', '') for item in history.get('topics', [])[-30:]]

    prompt = f"""You are a viral content strategist for "Mythica Report," a YouTube Shorts channel.

ANALYZING RAW TRENDING MYSTERY TOPICS:
{chr(10).join(f"- {t}" for t in trends[:30])}

**CRITICAL: AVOID RECENTLY COVERED TOPICS:**
{chr(10).join(f"- {title}" for title in previous_titles) if previous_titles else "None yet."}

üö® TITLE PATTERN REQUIREMENTS (RETENTION-CRITICAL):

‚úÖ GOOD PATTERNS (70%+ retention):
- "The [Role/Object] Who/That Vanished" (e.g., "The Hiker Who Vanished")
- "The [Mystery]: [Impossible Detail]" (e.g., "The Signal: Came From a Dead Star")
- "[Action] + [Impossible Outcome]" (e.g., "Five Planes Vanished Without Trace")

‚ùå BAD PATTERNS (20-35% retention):
- "[Name]: [Mystery]" (e.g., "Leah Roberts: The Vanishing Driver")
- "[Name] + [Action]" (e.g., "John Doe Disappeared in 2000")
- Reason: Viewers don't know the name, need context first

MANDATORY TITLE RULES:
1. MUST include "Vanished" or "Disappeared"
2. MUST NOT start with unfamiliar proper names
3. MUST describe ROLE before name (e.g., "The Student Who Vanished" not "Sarah Who Vanished")

SELECT TOP 5 TOPICS with retention-optimized titles.

OUTPUT (JSON):
{{
  "selected_topics": [
    {{
      "title": "The [Role/Thing] Who/That Vanished",
      "reason": "Why this is viral + follows proven retention pattern",
      "viral_score": 95,
      "story_hook": "First sentence revealing the mystery immediately",
      "core_mystery": "Central unanswered question",
      "ending_question": "Cliffhanger for comments"
    }}
  ]
}}
"""

    max_retries = 3
    for attempt in range(max_retries):
        try:
            response = model.generate_content(prompt)
            result_text = response.text.strip()
            
            json_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', result_text, re.DOTALL)
            if not json_match:
                json_match = re.search(r'\{.*\}', result_text, re.DOTALL)
            
            if json_match:
                json_str = json_match.group(1)
                data = json.loads(json_str)
            else:
                raise json.JSONDecodeError("No JSON found", result_text, 0)
                
            trending_ideas = []
            for item in data.get('selected_topics', [])[:5]:
                title = item.get('title', 'Unknown Mystery')
                
                # ‚úÖ VALIDATE TITLE PATTERN
                title_lower = title.lower()
                has_vanished = 'vanish' in title_lower or 'disappear' in title_lower
                
                if not has_vanished:
                    print(f"   ‚ö†Ô∏è Skipping '{title}' - missing 'vanished/disappeared'")
                    continue
                
                # Check for name-first pattern
                if ':' in title:
                    first_part = title.split(':')[0].strip()
                    words = first_part.split()
                    if len(words) <= 3 and all(w[0].isupper() for w in words if w):
                        print(f"   ‚ö†Ô∏è Skipping '{title}' - name-first pattern (low retention)")
                        continue
                
                # üö® v6.0.1: Fixed entity-based duplicate detection
                is_duplicate, matching_entities = has_duplicate_entity(title, entity_history)
                
                if is_duplicate:
                    print(f"   üö´ DUPLICATE BLOCKED: '{title}'")
                    print(f"      Entity match: {', '.join(matching_entities)}")
                    print(f"      This topic was already covered in last 90 days")
                    continue  # Skip this topic entirely
                
                trending_ideas.append({
                    "topic_title": title,
                    "summary": item.get('reason', 'High viral potential'),
                    "category": "Unsolved Mystery",
                    "viral_score": item.get('viral_score', 90),
                    "story_hook": item.get('story_hook', 'A chilling discovery...'),
                    "core_mystery": item.get('core_mystery', 'Unanswered question'),
                    "ending_question": item.get('ending_question', 'What do you think?'),
                })
            
            if not trending_ideas:
                print("‚ö†Ô∏è All topics were duplicates or rejected - retrying...")
                raise ValueError("All topics rejected by duplicate/pattern validation")

            print(f"‚úÖ Gemini ranked {len(trending_ideas)} UNIQUE topics (v6.0.1 entity-verified)")
            for i, idea in enumerate(trending_ideas, 1):
                entities = extract_entities_from_title(idea['topic_title'])
                entity_str = f" [{', '.join(list(entities)[:2])}]" if entities else " [no entities]"
                print(f"   {i}. [{idea['viral_score']}] {idea['topic_title'][:60]}{entity_str}")
            
            return trending_ideas
            
        except Exception as e:
            print(f"‚ùå Attempt {attempt + 1} failed: {e}")
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)
    
    print("‚ö†Ô∏è Gemini ranking failed, using fallback...")
    return get_fallback_mystery_ideas()


def get_fallback_mystery_ideas() -> List[Dict[str, Any]]:
    """Fallback ideas with CORRECT title patterns"""
    
    fallbacks = [
        {
            "topic_title": "The Ghost Ship of the Arctic: The Octavius",
            "summary": "Classic maritime mystery with strong retention pattern",
            "category": "Historical Mystery",
            "viral_score": 94,
            "story_hook": "In 1775, a whaling ship found a ghost ship frozen in Arctic ice.",
            "core_mystery": "How did the ship end up thousands of miles off course with its crew frozen?",
            "ending_question": "Was it a shortcut gone wrong, or something more sinister?"
        },
        {
            "topic_title": "The Signal That Came From a Dead Star",
            "summary": "Real astronomical event, high curiosity factor",
            "category": "Cosmic Mystery",
            "viral_score": 92,
            "story_hook": "Astronomers detected a repeating signal from a cosmic graveyard.",
            "core_mystery": "What is sending a signal from where nothing should exist?",
            "ending_question": "Is it natural, or something else?"
        },
        {
            "topic_title": "The Hiker Who Vanished From an Easy Trail",
            "summary": "Taps into national park disappearance mystery trend",
            "category": "True Crime",
            "viral_score": 95,
            "story_hook": "Hundreds vanish from US National Parks without a trace.",
            "core_mystery": "Why do experienced hikers disappear from easy trails?",
            "ending_question": "Are these accidents, or is something else happening?"
        }
    ]
    print(f"üìã Using {len(fallbacks)} retention-optimized fallback ideas")
    return fallbacks


def save_trending_data(trending_ideas: List[Dict[str, Any]]):
    """Save trending data to file"""
    
    trending_data = {
        "topics": [idea["topic_title"] for idea in trending_ideas],
        "full_data": trending_ideas,
        "generated_at": datetime.now().isoformat(),
        "timestamp": time.time(),
        "niche": "mystery_stories",
        "channel": "Mythica Report",
        "source": f"google_trends + reddit + youtube + evergreen + gemini_ranking (category: {MYSTERY_CATEGORY})",
        "version": "6.0.9_category_aware",
        "category": MYSTERY_CATEGORY  # üÜï Track which category was fetched
    }
    
    trending_file = os.path.join(TMP, "trending.json")
    with open(trending_file, "w", encoding="utf-8") as f:
        json.dump(trending_data, f, indent=2, ensure_ascii=False)
    
    print(f"\nüíæ Saved trending data to: {trending_file}")
    return trending_file


if __name__ == "__main__":
    real_trends = get_real_mystery_trends()
    history = load_history()
    
    if real_trends:
        trending_ideas = filter_and_rank_mystery_trends(real_trends, history)
    else:
        print("‚ö†Ô∏è No real trends, using fallback...")
        trending_ideas = get_fallback_mystery_ideas()
    
    if trending_ideas:
        print(f"\n" + "="*70)
        print(f"üîÆ TOP VIRAL MYSTERY IDEAS (RETENTION-OPTIMIZED v6.0.1)")
        print("="*70)
        
        for i, idea in enumerate(trending_ideas, 1):
            print(f"\nüíé IDEA {i}:")
            print(f"   Title: {idea['topic_title']}")
            print(f"   Viral Score: {idea.get('viral_score', 'N/A')}/100")
            print(f"   Hook: {idea.get('story_hook', 'N/A')}")
            print(f"   Mystery: {idea.get('core_mystery', 'N/A')}")
        
        save_trending_data(trending_ideas)
        
        print(f"\n‚úÖ TRENDING DATA READY (v6.0.1)")
        print(f"   Quality: Retention-optimized title patterns")
        print(f"   Validation: Name-first patterns rejected")
        print(f"   Duplicate Detection: Entity-based (fixed false positives)")
        
    else:
        print("\n‚ùå Could not retrieve any trending ideas")
        exit(1)